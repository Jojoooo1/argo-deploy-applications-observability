apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: postgres-rules
  namespace: observability
  labels:
    scrapWith: kube-prometheus-stack
spec:
  groups:
    - name: PostgresExporter
      rules:
        - alert: PostgresqlDown
          expr: "pg_up == 0"
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Postgresql down (instance {{ $labels.instance }})
            description: 'Postgresql instance is down VALUE = {{ $value }}'

        - alert: PostgresqlRestarted
          expr: "time() - pg_postmaster_start_time_seconds < 60"
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Postgresql restarted (instance {{ $labels.instance }})
            description: 'Postgresql restarted VALUE = {{ $value }}'

        - alert: PostgresqlExporterError
          expr: "pg_exporter_last_scrape_error > 0"
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Postgresql exporter error in pod {{ $labels.pod }}
            description: 'Postgresql exporter is showing errors. A query may be buggy in query.yaml VALUE = {{ $value }}'

        # - alert: PostgresqlTableNotAutoVacuumed
        #   expr: "(pg_stat_user_tables_last_autovacuum > 0) and (time() - pg_stat_user_tables_last_autovacuum) > 60 * 60 * 24 * 10"
        #   for: 0m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: Postgresql table not auto vacuumed in pod {{ $labels.pod }}
        #     description: 'Table {{ $labels.relname }} has not been auto vacuumed for 10 days VALUE = {{ $value }}'

        # - alert: PostgresqlTableNotAutoAnalyzed
        #   expr: "(pg_stat_user_tables_last_autoanalyze > 0) and (time() - pg_stat_user_tables_last_autoanalyze) > 24 * 60 * 60 * 10"
        #   for: 0m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: Postgresql table not auto analyzed in pod {{ $labels.pod }}
        #     description: 'Table {{ $labels.relname }} has not been auto analyzed for 10 days VALUE = {{ $value }}'

        - alert: PostgresqlTooManyConnections
          expr: 'sum by (datname) (pg_stat_activity_count{datname!~"template.*|postgres"}) > pg_settings_max_connections * 0.8'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Postgresql too many connections in pod {{ $labels.pod }}
            description: 'PostgreSQL pod has too many connections (> 80%). VALUE = {{ $value }}'

        # - alert: PostgresqlNotEnoughConnections
        #   expr: 'sum by (datname) (pg_stat_activity_count{datname!~"template.*|postgres"}) < 5'
        #   for: 2m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: Postgresql not enough connections in pod {{ $labels.pod }}
        #     description: 'PostgreSQL pod should have more connections (> 5) VALUE = {{ $value }}'

        - alert: PostgresqlDeadLocks
          expr: 'increase(pg_stat_database_deadlocks{datname!~"template.*|postgres"}[1m]) > 5'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Postgresql dead locks in pod {{ $labels.pod }}
            description: 'PostgreSQL has dead-locks VALUE = {{ $value }}'

        # - alert: PostgresqlHighRollbackRate
        #   expr: 'rate(pg_stat_database_xact_rollback{datname!~"template.*"}[5m]) / rate(pg_stat_database_xact_commit{datname!~"template.*"}[5m]) > 0.1'
        #   for: 0m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: Postgresql high rollback rate in pod {{ $labels.pod }}
        #     description: 'Ratio of transactions being aborted compared to committed is > 10 % VALUE = {{ $value }}'

        # - alert: PostgresqlCommitRateLow
        #   expr: "rate(pg_stat_database_xact_commit[1m]) < 10"
        #   for: 2m
        #   labels:
        #     severity: critical
        #   annotations:
        #     summary: Postgresql commit rate low in pod {{ $labels.pod }}
        #     description: 'Postgresql seems to be processing very few transactions VALUE = {{ $value }}'

        # - alert: PostgresqlLowXidConsumption
        #   expr: "rate(pg_txid_current[1m]) < 5"
        #   for: 2m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: Postgresql low XID consumption in pod {{ $labels.pod }}
        #     description: 'Postgresql seems to be consuming transaction IDs very slowly VALUE = {{ $value }}'

        - alert: PostgresqlHighRateStatementTimeout
          expr: 'rate(postgresql_errors_total{type="statement_timeout"}[1m]) > 3'
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Postgresql high rate statement timeout in pod {{ $labels.pod }}
            description: 'Postgres transactions showing high rate of statement timeouts VALUE = {{ $value }}'

        - alert: PostgresqlHighRateDeadlock
          expr: 'increase(postgresql_errors_total{type="deadlock_detected"}[1m]) > 1'
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Postgresql high rate deadlock in pod {{ $labels.pod }}
            description: 'Postgres detected deadlocks VALUE = {{ $value }}'

        - alert: PostgresqlConfigurationChanged
          expr: '{__name__=~"pg_settings_.*"} != ON(__name__) {__name__=~"pg_settings_([^t]|t[^r]|tr[^a]|tra[^n]|tran[^s]|trans[^a]|transa[^c]|transac[^t]|transact[^i]|transacti[^o]|transactio[^n]|transaction[^_]|transaction_[^r]|transaction_r[^e]|transaction_re[^a]|transaction_rea[^d]|transaction_read[^_]|transaction_read_[^o]|transaction_read_o[^n]|transaction_read_on[^l]|transaction_read_onl[^y]).*"} OFFSET 5m'
          for: 5m
          labels:
            severity: info
          annotations:
            summary: Postgresql configuration changed in pod {{ $labels.pod }}
            description: 'Postgres Database configuration change has occurred VALUE = {{ $value }}'

        - alert: PostgresqlSslCompressionActive
          expr: "sum(pg_stat_ssl_compression) > 0"
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Postgresql SSL compression active in pod {{ $labels.pod }}
            description: 'Database connections with SSL compression enabled. This may add significant jitter in replication delay. Replicas should turn off SSL compression via `sslcompression=0` in `recovery.conf`. VALUE = {{ $value }}'

        - alert: PostgresqlTooManyLocksAcquired
          expr: "((sum (pg_locks_count)) / (pg_settings_max_locks_per_transaction * pg_settings_max_connections)) > 0.20"
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Postgresql too many locks acquired in pod {{ $labels.pod }}
            description: 'Too many locks acquired on the database. If this alert happens frequently, we may need to increase the postgres setting max_locks_per_transaction. VALUE = {{ $value }}'

        - alert: PostgresqlBloatIndexHigh(>80%)
          expr: "pg_bloat_btree_bloat_pct > 80 and on (idxname) (pg_bloat_btree_real_size > 100000000)"
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: Postgresql bloat index high (> 80%) for index "{{ $labels.idxname }}"'
            description: 'The index {{ $labels.idxname }} is bloated. You should execute `REINDEX INDEX CONCURRENTLY {{ $labels.idxname }};` VALUE = {{ $value }}'

        - alert: PostgresqlBloatTableHigh(>80%)
          expr: "pg_bloat_table_bloat_pct > 80 and on (relname) (pg_bloat_table_real_size > 200000000)"
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: 'Postgresql bloat table high (> 80%) for table "{{ $labels.schemaname }}.{{ $labels.relname }}"'
            description: 'The table {{ $labels.relname }} is bloated. You should execute `VACUUM {{ $labels.relname }};` VALUE = {{ $value }}'

        - alert: PostgresqlTooManyDeadTuples
          expr: "pg_stat_user_tables_n_dead_tup > 150000"
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: 'Postgresql too many dead tuples for table "{{ $labels.schemaname }}.{{ $labels.relname }}"'
            description: 'PostgreSQL dead tuples is too large VALUE = {{ $value }}'

        - alert: PostgresqlUnusedReplicationSlot
          expr: 'pg_replication_slots_active{slot_name!~".*bigquery.*"} == 0'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: 'Postgresql unused replication slot "{{ $labels.slot_name }}"'
            description: 'Unused Replication Slots VALUE = {{ $value }}'
        
        # Datastream can disconnect for longer period this is why we need to exclude it from PostgresqlUnusedReplicationSlot
        - alert: PostgresqlDatastreamUnusedReplicationSlot
          expr: 'pg_replication_slots_active{slot_name=~".*bigquery.*"} == 0'
          for: 25m
          labels:
            severity: warning
          annotations:
            summary: 'Postgresql/Datastream unused replication slot "{{ $labels.slot_name }}"'
            description: 'Unused Replication Slots VALUE = {{ $value }}'

        - alert: PostgresReplicationLagIsTooBig
          expr: pg_replication_lag > 600
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: PostgreSQL replication is lagging by {{ $value }} seconds.
            description: 'replication for PostgreSQL is lagging by {{ $value }} seconds. '

        - alert: PostgresReplicationLagSizeTooLarge
          expr: pg_replication_slots_pg_wal_lsn_diff > 4e+09 # 4Gi
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: 'Postgres replication lag size is to large by {{ $value | humanize1024}}B'
            description: 'Replication lag size is currently {{ $value | humanize1024}}B behind the leader '
