apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector-traces
  namespace: observability
spec:
  # https://github.com/open-telemetry/opentelemetry-operator/blob/main/docs/api.md
  mode: deployment
  serviceAccount: collector
  env:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
          
    - name: GOMEMLIMIT
      value: 205MiB # should be 80% of memory limit

  autoscaler:
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilization: 80
    targetMemoryUtilization: 80

  resources:
    requests:
      memory: 256Mi
      cpu: 200m
    limits:
      memory: 256Mi
      cpu: 200m

  # https://www.otelbin.io/
  config: |
    # if you are using tempo disable tempo.metricsGenerator
    connectors:
      spanmetrics:

    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318

      # Collect own metrics
      prometheus:
        trim_metric_suffixes: true
        config:
          scrape_configs:
            - job_name: 'otel-collector-traces'
              scrape_interval: 5s
              static_configs:
                - targets: ["${env:MY_POD_IP}:8888"]

    processors:
      k8sattributes:
      k8sattributes/2:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: connection

      batch:
        timeout: 200ms

      # drop metrics if memory usage gets too high
      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 15

    exporters:
      debug:

      otlp:
        endpoint: tempo.observability:4317
        tls:
          insecure: true

      # Use this exporter if you want to send otlp metrics to prometheus.
      # otlphttp/prometheus:
      #   endpoint: http://kube-prometheus-stack-prometheus.observability:9090/api/v1/otlp
      #   tls:
      #     insecure: true

      prometheusremotewrite:
        endpoint: http://kube-prometheus-stack-prometheus.observability:9090/api/v1/write
        resource_to_telemetry_conversion:
          enabled: true
        tls:
          insecure: true

    extensions:
      health_check:
      pprof:
      zpages:

    service:
      # expose collector metrics on port 8888
      telemetry:
        logs:
          encoding: json
        metrics:
          level: detailed

      # add healthcheck, debugging port and pprof endpoints
      extensions: [health_check, zpages, pprof]

      pipelines:
        traces:
          receivers: [otlp]
          processors: [k8sattributes, k8sattributes/2, batch, memory_limiter]
          exporters: [otlp, spanmetrics]
        metrics:
          receivers: [prometheus, spanmetrics]
          processors: [k8sattributes, k8sattributes/2, batch, memory_limiter]
          exporters: [prometheusremotewrite, debug]