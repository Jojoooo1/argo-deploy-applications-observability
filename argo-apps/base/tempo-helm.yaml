apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: observability-tempo-helm
  namespace: argocd

  # Add this finalizer ONLY if you want these to cascade delete (A cascade delete, deletes both the app and its resources, rather than only the app.)
  # finalizers:
  #   - resources-finalizer.argocd.argoproj.io

spec:
  project: default
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground

  source:
    chart: tempo
    repoURL: "https://grafana.github.io/helm-charts"
    targetRevision: 1.7.2

    # https://github.com/grafana/helm-charts/blob/main/charts/tempo
    helm:
      valuesObject:
        fullnameOverride: tempo

        tempo:
          retention: 720h # 30d
          resources:
            requests: 
              memory: 1Gi
              cpu: 200m
            limits:
              memory: 1Gi
              cpu: 200m
            
          global_overrides:
            # In high peak scenario we need to increase this value.
            max_traces_per_user: 20000
          
          # https://github.com/grafana/tempo/blob/main/docs/tempo/website/configuration/_index.md#storage
          storage:
            trace:
              backend: local
              # backend: gcs    
              #  gcs:
              #    bucket_name: <bucket_name>

          # Disable if use otel.connectors.spanmetrics
          metricsGenerator:
            enabled: false
            remoteWriteUrl: "http://kube-prometheus-stack-prometheus.observability:9090/api/v1/write"

        # Necessary to override config if want to use spanmetrics (https://github.com/grafana/tempo/issues/3064)
        config: |
          multitenancy_enabled: {{ .Values.tempo.multitenancyEnabled }}
          usage_report:
            reporting_enabled: {{ .Values.tempo.reportingEnabled }}
          compactor:
            compaction:
              block_retention: {{ .Values.tempo.retention }}
          distributor:
            receivers:
              {{- toYaml .Values.tempo.receivers | nindent 8 }}
          ingester:
            {{- toYaml .Values.tempo.ingester | nindent 6 }}
          server:
            {{- toYaml .Values.tempo.server | nindent 6 }}
          storage:
            {{- toYaml .Values.tempo.storage | nindent 6 }}
          querier:
            {{- toYaml .Values.tempo.querier | nindent 6 }}
          query_frontend:
            {{- toYaml .Values.tempo.queryFrontend | nindent 6 }}
          overrides:
            {{- toYaml .Values.tempo.global_overrides | nindent 6 }}
            {{- if .Values.tempo.metricsGenerator.enabled }}
                metrics_generator_processors:
                - 'service-graphs'
                - 'span-metrics'
                - 'local-blocks'

          metrics_generator:
                storage:
                  path: "/tmp/tempo/generator/wal"
                  remote_write:
                    - url: {{ .Values.tempo.metricsGenerator.remoteWriteUrl }}
                traces_storage:
                  path: "/tmp/tempo/generator/traces"
            {{- end }}

        serviceMonitor:
          enabled: true
          namespace: observability
          additionalLabels:
            prometheus.io/scrap-with: kube-prometheus-stack

        # Tempo ingesters make heavy use of local disks to store wal and blocks before being flushed to the backend
        persistence:
          enabled: true
          size: 10Gi

        # serviceAccount:
        #   annotations:
        #     iam.gke.io/gcp-service-account: tempo-storage@${ARGOCD_ENV_PROJECT}.iam.gserviceaccount.com

  destination:
    server: "https://kubernetes.default.svc"
    namespace: observability
