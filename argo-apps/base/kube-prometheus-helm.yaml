apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: observability-kube-prometheus-stack-helm
  namespace: argocd

  # Add this finalizer ONLY if you want these to cascade delete (A cascade delete, deletes both the app and its resources, rather than only the app.)
  # finalizers:
  #   - resources-finalizer.argocd.argoproj.io

spec:
  project: default
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground

  sources:
    - repoURL: https://github.com/prometheus-community/helm-charts.git
      path: charts/kube-prometheus-stack/charts/crds/
      targetRevision: kube-prometheus-stack-57.1.0

    - chart: kube-prometheus-stack
      repoURL: "https://prometheus-community.github.io/helm-charts"
      targetRevision: 57.1.0
      
      # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
      helm:
        skipCrds: true
        valuesObject:

          fullnameOverride: "kube-prometheus-stack"

          ## Grafana ##
          grafana:
            fullnameOverride: grafana
            image:
              tag: 10.4.0

            # resources:
            #   requests: 
            #     memory: 128Mi
            #     cpu: 50m
            #   limits:
            #     memory: 384Mi
            #     cpu: 100m
            
            serviceMonitor:
              enabled: true
              labels:
                prometheus.io/scrap-with: kube-prometheus-stack

            # Ingress
            ingress:
              enabled: true
              ingressClassName: nginx
              hosts: 
                - grafana${ARGOCD_ENV_DNS_ENV}.${ARGOCD_ENV_DNS_DOMAIN}
            
            # Configurations
            adminUser: admin
            adminPassword: password
            grafana.ini:
              feature_toggles:
                enable: panelTitleSearch nestedFolders lokiExperimentalStreaming storage traceToMetrics lokiQuerySplittingConfig lokiFormatQuery traceQLStreaming metricsSummary featureToggleAdminPage enableNativeHTTPHistogram prometheusPromQAIL logsInfiniteScrolling enablePluginsTracingByDefault scenes extraThemes dashgpt

            # Dashboards
            defaultDashboardsTimezone: browser
            dashboardProviders:
              dashboardproviders.yaml:
                apiVersion: 1
                providers:
                  - name: 'grafana-dashboards-kubernetes'
                    orgId: 1
                    folder: 'Kubernetes-v2'
                    type: file
                    options:
                      path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes        
            dashboards:
              grafana-dashboards-kubernetes:
                k8s-views-global:
                  url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json
                k8s-views-namespaces:
                  url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json
                k8s-views-nodes:
                  url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json
                k8s-views-pods:
                  url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json

            # Sidecar configurations (used to inject dashboard via configmap)
            sidecar: 
              dashboards:
                # Allow dashboards to be consolidated into folders for proper visualisation in Grafana.
                folderAnnotation: grafana_folder
                provider:
                  foldersFromFilesStructure: true
                label: grafana_dashboard
                labelValue: kube-prometheus-stack
                # Puts kube-prometheus-stack default dashboards in 'Kubernetes' folder
                annotations:
                  grafana_folder: /tmp/dashboards/Kubernetes
                  
              datasources:
                exemplarTraceIdDestinations:
                  datasourceUid: tempo
                  traceIdLabelName: trace_id

            # Datasources
            additionalDataSources:
              # More config at https://grafana.com/docs/grafana/latest/datasources/tempo/configure-tempo-data-source/
              - name: Tempo
                uid: tempo
                type: tempo
                access: proxy
                url: http://tempo.observability.svc:3100
                jsonData:
                  # tracesToLogsV2:
                  #   datasourceUid: 'loki'
                  #   spanStartTimeShift: '1h'
                  #   spanEndTimeShift: '-1h'
                  #   tags: [{ key: 'service.name', value: 'app' }]
                  #   filterByTraceID: false
                  #   filterBySpanID: false
                  #   customQuery: true
                  #   query: '{$$$${__tags}} |="$$$${__span.traceId}"'
                  tracesToMetrics:
                    datasourceUid: 'prometheus'
                    spanStartTimeShift: '1h'
                    spanEndTimeShift: '-1h'
                    tags: [{ key: 'service.name', value: 'service' }]
                    queries:
                      - name: 'latency'
                        query: 'sum(rate(traces_spanmetrics_latency_bucket{$$__tags}[5m]))'
                  # traceToProfiles:
                  #   datasourceUid: 'grafana-pyroscope-datasource'
                  #   tags: ['job', 'instance', 'pod', 'namespace']
                  #   profileTypeId: 'process_cpu:cpu:nanoseconds:cpu:nanoseconds'
                  #   customQuery: true
                  #   query: 'method="${__span.tags.method}"'
                  serviceMap:
                    datasourceUid: 'prometheus'
                  nodeGraph:
                    enabled: true
                  search:
                    hide: false
                  # lokiSearch:
                  #   datasourceUid: 'loki'
                  traceQuery:
                    timeShiftEnabled: true
                    spanStartTimeShift: '1h'
                    spanEndTimeShift: '-1h'
                  spanBar:
                    type: 'Tag'
                    tag: 'http.path'
                    
              # - name: Loki
              #   uid: loki
              #   type: loki
              #   access: proxy
              #   url: http://loki-gateway.observability.svc
              #   jsonData:
              #     timeout: 1200
              #     derivedFields:
              #       - datasourceName: Tempo
              #         datasourceUid: tempo
              #         matcherRegex: ([A-Za-z0-9]{32})
              #         name: TraceId
              #         url: "$$$${__value.raw}"
              #         urlDisplayLabel: 'View Trace'

              # - name: Pyroscope
              #   uid: pyroscope
              #   type: grafana-pyroscope-datasource
              #   url: http://pyroscope.observability.svc:4040


          ## Prometheus ##
          prometheus:

            serviceMonitor:
              enabled: true
              additionalLabels:
                prometheus.io/scrap-with: kube-prometheus-stack

            podDisruptionBudget:
              enabled: true

            ingress:
              enabled: true
              ingressClassName: nginx
              hosts: 
                - prometheus${ARGOCD_ENV_DNS_ENV}.${ARGOCD_ENV_DNS_DOMAIN}

            # Configurations
            prometheusSpec:
              externalUrl: http://prometheus${ARGOCD_ENV_DNS_ENV}.${ARGOCD_ENV_DNS_DOMAIN}
              retention: "30d" # keep metrics for 30 days

              # resources:
              #   requests: 
              #     memory: 3Gi
              #     cpu: 550m
              #   limits:
              #     memory: 3Gi
              #     cpu: 550m
              
              storageSpec: 
                volumeClaimTemplate:
                  spec:
                    # storageClassName: standard-rwo
                    accessModes:
                    - ReadWriteOnce
                    resources:
                      requests: 
                        storage: 1Gi

              # Necessary if using Otel collector.
              enableRemoteWriteReceiver: true # /api/v1/otlp
              # https://prometheus.io/docs/prometheus/latest/feature_flags/
              enableFeatures:
                - exemplar-storage # enable traces in metrics.
                - memory-snapshot-on-shutdown
                - otlp-write-receiver # allow to write otel metrics to /api/v1/otlp

              ## Prometheus CRD Selectors ##
              ruleSelectorNilUsesHelmValues: false
              ruleSelector:
                matchLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack
              
              serviceMonitorSelectorNilUsesHelmValues: false
              serviceMonitorSelector:
                matchLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack

              podMonitorNamespaceSelector: false
              podMonitorSelector:
                matchLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack

              probeSelectorNilUsesHelmValues: false
              probeSelector:
                matchLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack

              scrapeConfigSelectorNilUsesHelmValues: false
              scrapeConfigSelector:
                matchLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack  

              alertmanagerConfigSelector:
                matchLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack

          prometheusOperator:
            # resources:
            #   requests:
            #     cpu: 100m
            #     memory: 100Mi
            #   limits:
            #     cpu: 200m
            #     memory: 200Mi

            serviceMonitor:
              enabled: true
              additionalLabels:
                prometheus.io/scrap-with: kube-prometheus-stack


          ## Alertmanager ##
          alertmanager:

            serviceMonitor:
              enabled: true
              additionalLabels:
                prometheus.io/scrap-with: kube-prometheus-stack

            ingress:
              enabled: true
              ingressClassName: nginx
              hosts: 
                - alertmanager${ARGOCD_ENV_DNS_ENV}.${ARGOCD_ENV_DNS_DOMAIN}
            
            # Configurations
            alertmanagerSpec:
              externalUrl: http://alertmanager${ARGOCD_ENV_DNS_ENV}.${ARGOCD_ENV_DNS_DOMAIN}
              retention: "720h" # 30 days

              # resources:
              #   requests: 
              #     memory: 128Mi
              #     cpu: 100m
              #   limits:
              #     memory: 128Mi
              #     cpu: 100m

              storage: 
                volumeClaimTemplate:
                  spec:
                    # update if you want a better storage class.
                    # storageClassName: standard-rwo
                    accessModes:
                    - ReadWriteOnce
                    resources:
                      requests: 
                        storage: 1Gi

            # https://prometheus.io/docs/alerting/latest/configuration/#configuration
            config:
              # Try your configuration with https://prometheus.io/webtools/alerting/routing-tree-editor/
              route:
                receiver: 'slack' # default receiver
                routes:
                  # Ensure alerting pipeline is functional
                  - receiver: 'null'
                    matchers:
                      - alertname = "Watchdog"
                  # Redirect all severity to slack
                  - receiver: 'slack'
                    matchers:
                      - severity =~ info|critical|warning

              receivers:
                - name: 'null'
                
                # Slack configurations
                - name: slack
                  slack_configs:
                    - send_resolved: true
                      channel: '#danger-room-sandbox'
                      username: 'Alertmanager'
                      api_url: https://hooks.slack.com/services/your-slack-url
                      icon_url: https://avatars3.githubusercontent.com/u/3380462

                      # Alert template
                      title: |
                        [{{ .Status | toUpper -}}
                        {{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{- end -}}
                        ] {{ .CommonLabels.alertname }}
                      text: |-
                        {{ range .Alerts -}}
                        *Severity:* `{{ .Labels.severity }}`
                        *Summary:* {{ .Annotations.summary }}
                        *Description:* {{ .Annotations.description }}
                        *Details:*
                          • *env:* `${ARGOCD_ENV_ENV}`
                          {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
                          {{ end }}
                        {{ end }}
                      actions:
                        # Can add dashboard_url, runbook_url [...]
                        - type: button
                          text: 'Runbook :green_book:'
                          url: '{{ (index .Alerts 0).Annotations.runbook_url }}'
                        - type: button
                          text: 'Query :mag:'
                          url: '{{ (index .Alerts 0).GeneratorURL }}'
                        - type: button
                          text: 'Silence :no_bell:'
                          url: |
                            {{ .ExternalURL }}/#/silences/new?filter=%7B
                            {{- range .CommonLabels.SortedPairs -}}
                                {{- if ne .Name "alertname" -}}
                                    {{- .Name }}%3D"{{- .Value -}}"%2C%20
                                {{- end -}}
                            {{- end -}}
                            alertname%3D"{{- .CommonLabels.alertname -}}"%7D

          ## Stack ##
          
          # type: Daemonset -> collects metrics from the host system
          nodeExporter:
            enabled: true
          prometheus-node-exporter:
            fullnameOverride: prometheus-node-exporter

            # resources:
            #   requests: 
            #     memory: 32Mi
            #     cpu: 60m
            #   limits:
            #     memory: 64Mi
            #     cpu: 120m
                
            prometheus:
              monitor:
                enabled: true
                additionalLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack
                  

          # type: Deployment -> Listens to the Kubernetes API server and generates metrics about the state of the objects such as deployments, nodes and pods.
          kubeStateMetrics:
            enabled: true
          kube-state-metrics:
            fullnameOverride: kube-state-metrics

            # resources:
            #   requests: 
            #     memory: 32Mi
            #     cpu: 25m
            #   limits:
            #     memory: 64Mi
            #     cpu: 50m

            prometheus:
              monitor:
                enabled: true
                additionalLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack


          ## Node component: https://kubernetes.io/docs/concepts/overview/components/#node-components ##
          
          # Manage deployment of pods to Kubernetes nodes. It receives commands from the API server and instructs the container runtime to start/stop containers
          # It pulls node, pod, and container metrics
          kubelet: 
            # k3s: sudo curl --insecure https://localhost:10250/metrics --cacert /var/lib/rancher/k3s/server/tls/server-ca.crt --cert /var/lib/rancher/k3s/server/tls/client-admin.crt --key /var/lib/rancher/k3s/server/tls/client-admin.key
            enabled: true
            serviceMonitor:
              enabled: true
              additionalLabels:
                prometheus.io/scrap-with: kube-prometheus-stack

          # Network proxy that runs on each node that maintains the network rules.
          # kube-proxy
          kubeProxy:
            enabled: false
            serviceMonitor:
              enabled: false
              additionalLabels:
                prometheus.io/scrap-with: kube-prometheus-stack


          ## Controle plane components: https://kubernetes.io/docs/concepts/overview/components/#control-plane-components ## 
          ## k3s https://docs.k3s.io/installation/requirements#inbound-rules-for-k3s-server-nodes ##

          # kubernetes API
          kubeApiServer:
            # k3s: sudo curl --insecure https://localhost:6443/metrics --cacert /var/lib/rancher/k3s/server/tls/server-ca.crt --cert /var/lib/rancher/k3s/server/tls/client-admin.crt --key /var/lib/rancher/k3s/server/tls/client-admin.key
            enabled: false

          # Runs controller processes, helps maintain the desired state of resources and reacts to changes in the cluster, ensuring that the actual state aligns with the declared configuration.
          kubeControllerManager:
            # k3s: sudo curl --insecure https://localhost:10257/metrics --cacert /var/lib/rancher/k3s/server/tls/server-ca.crt --cert /var/lib/rancher/k3s/server/tls/client-admin.crt --key /var/lib/rancher/k3s/server/tls/client-admin.key
            enabled: false

          # Watches for newly created Pods with no assigned node, and selects a node for them to run on.
          kubeScheduler:
            # k3s: sudo curl --insecure https://localhost:10259/metrics --cacert /var/lib/rancher/k3s/server/tls/server-ca.crt --cert /var/lib/rancher/k3s/server/tls/client-admin.crt --key /var/lib/rancher/k3s/server/tls/client-admin.key
            enabled: false
          
          coreDns:
            enabled: false
          kubeDns:
            enabled: false
          kubeEtcd:
            enabled: false
          
          defaultRules:
            labels:
              prometheus.io/scrap-with: kube-prometheus-stack

            rules:
              # disable components that are not exposed in controle-plane in managed environment.
              kubeApiserverAvailability: false
              kubeApiserverBurnrate: false
              kubeApiserverHistogram: false
              kubeApiserverSlos: false
              etcd: false
              kubeControllerManager: false
              kubeSchedulerAlerting: false
              kubeSchedulerRecording: false
            
  destination:
    server: "https://kubernetes.default.svc"
    namespace: observability
